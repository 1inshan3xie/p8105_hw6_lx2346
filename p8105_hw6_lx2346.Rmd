---
title: "p8105_hw6_lx2346"
author: "linshan"
date: "2024-11-17"
output: github_document
---
Load the packages
```{r}
library(tidyverse)
library(modelr)
```


## Problem 1
Use the code chunk below to download the data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
``` 

Generate 5000 bootstrap samples.
```{r}
boot_straps = 
  weather_df |>
  modelr::bootstrap(5000)|>
  mutate(
    strap = map(strap, as_tibble),
    models = map(strap, \(df) lm(tmax ~ tmin, data = df))) 
```

Clean the result and produce estimates of 𝑟̂2 ,𝛽̂0 and 𝛽
```{r}
weather_result = 
  boot_straps |>
  mutate(result_1 = map(models, broom::tidy)) |>
  unnest(result_1) |>
  select(.id, models, term, estimate) |>
  pivot_wider(
    id_cols = c(.id, models),
    names_from = term,
    values_from = estimate
  ) |>
  select(.id, models,
    beta0 = `(Intercept)`,
    beta1 = tmin) |>
  mutate(
    log_b0_b1 = log(beta0 * beta1)
  ) |>
  mutate(result_2 = map(models, broom::glance)) |>
  unnest(result_2) |>
  select(.id, beta0, beta1, log_b0_b1, r.squared)
```
## Plot the distribution of your estimates, and describe these in words
```{r}
weather_result |>
  ggplot(aes(x = r.squared)) + 
  geom_histogram(binwidth = 0.01, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Bootstrap Distribution of R-squared", x = "R-squared")

weather_result |>
  ggplot(aes(x = log_b0_b1)) + 
  geom_histogram(binwidth = 0.01, fill = "pink", color = "black") +
  theme_minimal() +
  labs(title = "Bootstrap Distribution of log(β0 * β1)", x = "log(β0 * β1)")
```
\
**Comment**: It can be observed that the R-squared values are concentrated around 0.9, approximately forming a bell shape. The distribution of log(β₀ * β₁) ranges from 1.90 to 2.10, with a peak around 2.01, also exhibiting a bell-shaped pattern.

## Identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for 𝑟̂2 and log(𝛽̂0∗𝛽̂1).
```{r}
ci_r2 = quantile(weather_result$r.squared, c(0.025, 0.975))
ci_log_beta = quantile(weather_result$log_b0_b1, c(0.025, 0.975))
tibble(
  ci_r2 = ci_r2,
  ci_log_beta = ci_log_beta)
```

## Problem 2
Import the data
```{r}
homicides = read_csv("./data/homicide-data.csv", na = c(".", "", "NA"))
```
Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved.
```{r}
homicides = homicides |>
  mutate(city_state = paste(city, state, sep = ", "),
         solve = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1))
```
Omit cities & Limit victim_race in white or black & Be sure that victim_age is numeric
```{r}
homicides = homicides |>
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age, na.rm =TRUE))
```
### Test for Baltimore, MD
```{r}
baltimore = homicides |>
  filter(city_state == "Baltimore, MD")
model_baltimore = glm(solve ~ victim_age + victim_sex + victim_race, data = baltimore, family = binomial)
baltimore_tidy = broom::tidy(model_baltimore) |>
  mutate(odds_ratio = exp(estimate))
ci = exp(confint(model_baltimore))
ci_sex = ci["victim_sexMale", ]
sex_or = baltimore_tidy |>
  filter(term == "victim_sexMale") |>
  pull(odds_ratio) 

tibble(sex_or = sex_or,
       sex_ci_ll = ci_sex[1],
       sex_ci_ul = ci_sex[2])

```
### Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 
```{r}
glm_data = homicides |>
  select(solve, victim_race, victim_age, victim_sex, city_state) |>
  group_by(city_state) |>
  nest()
```

```{r}
odd_ci = function(x) {
  model_tidy = broom::tidy(x) |>
    mutate(odds_ratio = exp(estimate))
  
  ci = exp(confint(x))
  
  ci_sex = ci["victim_sexMale",]
  
  sex_or = model_tidy |>
    filter(term == "victim_sexMale") |>
    pull(odds_ratio)
  
  out_df = tibble(
    sex_or = sex_or,
    sex_ci_ll = ci_sex[1],
    sex_ci_ul = ci_sex[2]
  )
  
}
```



```{r}
glm_model = glm_data |>
  mutate(
    model = map(data, \(df) glm(solve ~ victim_race + victim_age + victim_sex, data = df, family = binomial)),
    result = map(model, odd_ci)) |>
  select(city_state, result) |>
  unnest(result)

glm_model |>
  knitr::kable()
```

### Create a plot that shows the estimated ORs and CIs for each city. 
```{r}
glm_model |>
  ggplot( aes(x = reorder(city_state, sex_or), y = sex_or)) +
  geom_point(size = 1.5, color = "#4c6d9a") + 
  geom_errorbar(aes(ymin = sex_ci_ll, 
                    ymax = sex_ci_ul), 
                width = 1, color = "#dca199") + 
  labs(x = "City", 
       y = "Adjusted Odds Ratio (Male vs Female)",
       title = "Estimates and Confidence Intervals of the Odds Ratio by City",
       caption = "Odds ratios for solving homicides comparing male victims to female victims") +
  theme_minimal() +
  coord_flip() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))
```
\
**Comment**: The probability of solving homicide cases differs between male and female victims across different cities. In most cities, the probability of solving homicide cases for male victims is lower than that for female victims. However, in only six cities—Albuquerque, Atlanta, Fresno, Nashville, Stockton, and Richmond—the probability of solving homicide cases for male victims is higher than that for female victims.

## Problem 3
### Load and clean the data for regression analysis
```{r}
birthweight = read_csv("./data/birthweight.csv", na = c(".", "", "NA")) |>
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8), labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present"))
  )

colSums(is.na(birthweight))
```
\
The result shows there is no missing data in the dataset!

### Propose a regression model for birthweight.

In this analysis, I chose stepwise regression to identify the predictors of birthweight. Stepwise regression is a data-driven approach that iteratively evaluates the significance of variables, ensuring that only those with substantial predictive power are included in the final model. 

```{r}
full_model = lm(bwt ~ ., data = birthweight)
stepwise_model = step(full_model, direction = "both")
summary(stepwise_model)
```
\
Accordeing to the result of stepwise regression, the model i choose is lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken). It has the adjusted R-squared being 0.7173, the standard error of residual is 272.3
```{r}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)
summary(my_model)
```
### Show a plot of model residuals against fitted values
```{r}
birthweight = birthweight |>
  add_predictions(my_model, var = "pred_my_model") |>
  add_residuals(my_model, var = "resi_my_model")

birthweight |>
  ggplot(aes(x = pred_my_model, y = resi_my_model)) +
  geom_point(size = .8, alpha = .3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

### Construct the other two models
\
1. Use CV to select models
```{r}
cv_df =
  crossv_mc(birthweight, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

2. Apply to three different models and extract RMSE
```{r}
cv_res_df = cv_df |>
  
  mutate(
    my_mod  = map(train, \(x) lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = x)),
    control_mod_1 = map(train, \(x) lm(bwt ~ blength + gaweeks, data = x)),
    control_mod_2 = map(train, \(x) lm(bwt ~ bhead * blength * babysex, data = x))) |> 
  
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_control_mod_1 = map2_dbl(control_mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_control_mod_2 = map2_dbl(control_mod_2, test, \(mod, df) rmse(model = mod, data = df))) 

```
3. Make plots to make comparisions across models.
```{r}
cv_res_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin(alpha = 0.5) +
  scale_fill_manual(values = c("my_mod" = "#4C9BCF", "control_mod_1" = "#F1A340", "control_mod_2" = "#E49A8E")) +
  stat_summary(fun = "median", geom = "point", color = "black", size = 2, shape = 18) + 
  labs(
    title = "RMSE Distribution by Model Type",
    x = "Model Type",
    y = "Root Mean Squared Error (RMSE)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
  
```
\
**Comment**: Based on the RMSE violin plot, it can be observed that my_mod has the smallest RMSE, while control_mod_1 has the largest RMSE. This indicates that, in terms of prediction performance, my_mod performs better than control_mod_2, which in turn performs better than control_mod_1.
